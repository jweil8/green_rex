{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnlzer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "source_path = 'test_data'\n",
    "#source_file = 'test_strains_reviews.csv'\n",
    "target_path = '/Users/jordanweil/green_rex/test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"reviews = pd.read_csv('test_data/test_strains_reviews.csv')\\nr = reviews.set_index('Unnamed: 0')\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"reviews = pd.read_csv('test_data/test_strains_reviews.csv')\n",
    "r = reviews.set_index('Unnamed: 0')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews2(l):\n",
    "    \"\"\"Pass in a list of URL's and return them in a mongo db table as a dicitonary with \n",
    "    {'url', 'html'} and their corresponding values\"\"\"\n",
    "    r = requests.get(l)\n",
    "    html = (r.content)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_stars_list(d):\n",
    "#     stars = []\n",
    "#     for key, values in d.items():\n",
    "#         soup = BeautifulSoup(values, 'html.parser')\n",
    "#         tags = soup.select(\"div.div.stars\")\n",
    "#         for t in tags:\n",
    "#             stars.append(t.attrs['style'])\n",
    "#     return stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_int_conv(s):\n",
    "    star = (int((s[6:].split(';')[0]).strip('px'))/22)\n",
    "    \n",
    "    return star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_o_strains(i):\n",
    "    LOS = []\n",
    "    Type = []\n",
    "    r = requests.get(i)\n",
    "    soup2 = BeautifulSoup(r.content, 'html.parser')\n",
    "    strains = soup2.find_all('a', class_=\"ga_Explore_Strain_Tile\")\n",
    "    for s in strains:\n",
    "        LOS.append(str(s.attrs['href'])[8:])\n",
    "        Type.append(str(s.attrs['href'])[1:7])\n",
    "    z = list(zip(LOS,Type))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strain_dict_entry(strain, stype, user_id, userstars, userreview, straindict):\n",
    "    if straindict is None:\n",
    "        straindict = {}\n",
    "        \n",
    "    if strain not in straindict:\n",
    "        straindict[strain] = {\n",
    "            \"stype\" : stype,\n",
    "            \"user_rev\" : [] \n",
    "        }\n",
    "        \n",
    "    straindict[strain]['user_rev'].append({'user':user_id,\n",
    "                                         'stars':userstars,\n",
    "                                         'review':userreview\n",
    "                                        })\n",
    "    return straindict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs2(d):\n",
    "    \"\"\"Parse the HTML docs that we have stored in a dictionary, return as a list.\n",
    "    Also scrape and parse star rating for each review \"\"\"\n",
    "    strain_text= []\n",
    "    star_rate = []\n",
    "    user_name = []\n",
    "    \n",
    "    soup = BeautifulSoup(d, 'html.parser')\n",
    "    revs = soup.find_all('p',class_='strain-review__text') \n",
    "    for r in revs:\n",
    "        text = r.text\n",
    "        remove_punch = re.sub('[^A-Za-z ]' , \"\" ,text )\n",
    "        token = remove_punch.lower().split()\n",
    "        srm_token = [wnlzer.lemmatize(i) for i in token if not i in set(stopwords.words('english'))]\n",
    "        clean_text = \" \".join(srm_token)\n",
    "        strain_text.append(clean_text)\n",
    "        \n",
    "    tags = soup.select(\"div.div.stars\")\n",
    "    for t in tags:\n",
    "        star = t.attrs['style']\n",
    "        star_rate.append(star_int_conv(star))\n",
    "    \n",
    "    users = soup.find_all('div', class_='strain-review__title')\n",
    "    for u in users:\n",
    "        temp = u.find('h2')\n",
    "        user_name.append(temp.text)\n",
    "        \n",
    "    return star_rate, strain_text, user_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_dummy1(los):\n",
    "    \"\"\"pass in a list of strains to be scraped from Leafly.\n",
    "    returns dictionary keyed by strains w/strain info(reviews) as values\"\"\"\n",
    "    cnt = 0\n",
    "    strain_dict = None\n",
    "    for s in los:\n",
    "        strain = s[0]\n",
    "        stype = s[1]\n",
    "        url = \"https://www.leafly.com/{}/{}/reviews?page=\".format(stype, strain)\n",
    "    \n",
    "        for i in range(1,100):\n",
    "            rev_url=url+str(i)\n",
    "            d = get_reviews2(rev_url)\n",
    "            star, reviews, users = parse_docs2(d)\n",
    "            #print(star)\n",
    "            if len(star) == 0:\n",
    "                break\n",
    "\n",
    "            for userstars, userreview ,user_id in zip(star, reviews, users):\n",
    "                strain_dict = strain_dict_entry(strain, stype, user_id, userstars, userreview, strain_dict)\n",
    "                \n",
    "            \n",
    "            if cnt % 10 == 0:\n",
    "                print(cnt)\n",
    "            cnt +=1\n",
    "            \n",
    "    return strain_dict         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strains = list_o_strains('https://www.leafly.com/explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue-dream', 'hybrid'),\n",
       " ('sour-diesel', 'sativa'),\n",
       " ('gsc', 'hybrid'),\n",
       " ('green-crack', 'sativa'),\n",
       " ('og-kush', 'hybrid'),\n",
       " ('granddaddy-purple', 'indica'),\n",
       " ('original-glue', 'hybrid'),\n",
       " ('white-widow', 'hybrid'),\n",
       " ('blue-dream', 'hybrid'),\n",
       " ('sour-diesel', 'sativa'),\n",
       " ('gsc', 'hybrid'),\n",
       " ('green-crack', 'sativa'),\n",
       " ('og-kush', 'hybrid'),\n",
       " ('granddaddy-purple', 'indica'),\n",
       " ('original-glue', 'hybrid'),\n",
       " ('white-widow', 'hybrid'),\n",
       " ('jack-herer', 'sativa'),\n",
       " ('bubba-kush', 'indica'),\n",
       " ('pineapple-express', 'hybrid'),\n",
       " ('trainwreck', 'hybrid'),\n",
       " ('ak-47', 'hybrid'),\n",
       " ('durban-poison', 'sativa'),\n",
       " ('northern-lights', 'indica'),\n",
       " ('headband', 'hybrid'),\n",
       " ('blue-cheese', 'indica'),\n",
       " ('strawberry-cough', 'sativa'),\n",
       " ('chemdawg', 'hybrid'),\n",
       " ('purple-kush', 'indica'),\n",
       " ('lemon-haze', 'sativa'),\n",
       " ('super-lemon-haze', 'sativa'),\n",
       " ('grape-ape', 'indica'),\n",
       " ('blueberry', 'indica'),\n",
       " ('alaskan-thunder-fuck', 'sativa'),\n",
       " ('super-silver-haze', 'sativa'),\n",
       " ('blackberry-kush', 'indica'),\n",
       " ('cherry-pie', 'hybrid'),\n",
       " ('master-kush', 'indica'),\n",
       " ('skywalker-og', 'hybrid'),\n",
       " ('cheese', 'hybrid'),\n",
       " ('death-star', 'indica'),\n",
       " ('chocolope', 'sativa'),\n",
       " ('amnesia-haze', 'sativa'),\n",
       " ('tahoe-og', 'hybrid'),\n",
       " ('maui-wowie', 'sativa'),\n",
       " ('platinum-gsc', 'hybrid'),\n",
       " ('harlequin', 'sativa'),\n",
       " ('gods-gift', 'indica'),\n",
       " ('la-confidential', 'indica'),\n",
       " ('agent-orange', 'hybrid'),\n",
       " ('purple-urkle', 'indica'),\n",
       " ('lemon-kush', 'hybrid'),\n",
       " ('mazar-x-blueberry', 'hybrid'),\n",
       " ('golden-goat', 'hybrid'),\n",
       " ('afghan-kush', 'indica'),\n",
       " ('dutch-treat', 'hybrid'),\n",
       " ('hindu-kush', 'indica')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n"
     ]
    }
   ],
   "source": [
    "d = scraper_dummy1(test_strains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in d.keys():\n",
    "    print(k,len(d[k]['user_rev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['gsc']['user_rev'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files(d):\n",
    "    cnt = 0\n",
    "    for strain in d.keys():\n",
    "\n",
    "        if not os.path.isdir(os.path.join(target_path, strain)):\n",
    "             os.mkdir(os.path.join(target_path, strain))\n",
    "\n",
    "        for rev in d[strain]['user_rev']:\n",
    "\n",
    "            with open(os.path.join(target_path, strain, strain+'{:05d}.txt'.format(cnt)), mode='w') as strain_file:\n",
    "                strain_file.write(rev['review'])\n",
    "\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_files(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = \"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your]\".split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples 1463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['your'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "    dataset = load_files(target_path, shuffle=False)\n",
    "    print('n_samples {}'.format(len(dataset.data)))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    clf = Pipeline([\n",
    "        ('vect',TfidfVectorizer(analyzer='character', stop_words=stopwords_)),\n",
    "        ('clf', Perceptron())\n",
    "    ])\n",
    "\n",
    "    # TfidfVectorizer().get_params()\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 5), (1,10),(1,20)],\n",
    "        'vect__analyzer': ['word']#,'char']\n",
    "    }\n",
    "    gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "    cclf = gs_clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__analyzer': 'word', 'vect__ngram_range': (1, 5)}\n",
      "0.4808743169398907\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      ".ipynb_checkpoints       0.00      0.00      0.00         1\n",
      "        blue-dream       0.45      0.64      0.53       111\n",
      "       green-crack       0.38      0.17      0.24        52\n",
      "               gsc       0.58      0.35      0.43        75\n",
      "       sour-diesel       0.56      0.55      0.55       127\n",
      "\n",
      "         micro avg       0.48      0.48      0.48       366\n",
      "         macro avg       0.39      0.34      0.35       366\n",
      "      weighted avg       0.50      0.48      0.47       366\n",
      "\n",
      "[[ 0  1  0  0  0]\n",
      " [ 4 71  5  7 24]\n",
      " [ 4 17  9  4 18]\n",
      " [ 2 30  3 26 14]\n",
      " [ 3 39  7  8 70]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = gs_clf.predict(docs_test)\n",
    "\n",
    "print(gs_clf.best_params_)\n",
    "print(cclf.score(docs_test, y_test))\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm, cmap=plt.cm.cool)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class of trippy  fun, hyphy is blue-dream\n",
      "The class of high, smile, amazing is blue-dream\n",
      "The class of happy, hike, awake is green-crack\n",
      "The class of giggly, happy, fun is .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    u'trippy  fun, hyphy',\n",
    "    u'high, smile, amazing',\n",
    "    u'happy, hike, awake',\n",
    "    u'giggly, happy, fun'\n",
    "]\n",
    "\n",
    "predicted = cclf.predict(test_strings)\n",
    "\n",
    "for s, p in zip(test_strings, predicted):\n",
    "    print(u'The class of {} is {}'.format(s, dataset.target_names[p]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
