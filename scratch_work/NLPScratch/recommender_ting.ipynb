{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnlzer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#import parfit.parfit as pf\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "source_path = 'test_data'\n",
    "#source_file = 'test_strains_reviews.csv'\n",
    "target_path = '/Users/jordanweil/green_rex/test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"reviews = pd.read_csv('test_data/test_strains_reviews.csv')\\nr = reviews.set_index('Unnamed: 0')\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"reviews = pd.read_csv('test_data/test_strains_reviews.csv')\n",
    "r = reviews.set_index('Unnamed: 0')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews2(l):\n",
    "    \"\"\"Pass in a list of URL's and return them in a mongo db table as a dicitonary with \n",
    "    {'url', 'html'} and their corresponding values\"\"\"\n",
    "    r = requests.get(l)\n",
    "    html = (r.content)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_stars_list(d):\n",
    "#     stars = []\n",
    "#     for key, values in d.items():\n",
    "#         soup = BeautifulSoup(values, 'html.parser')\n",
    "#         tags = soup.select(\"div.div.stars\")\n",
    "#         for t in tags:\n",
    "#             stars.append(t.attrs['style'])\n",
    "#     return stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_int_conv(s):\n",
    "    star = (int((s[6:].split(';')[0]).strip('px'))/22)\n",
    "    return star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_o_strains(i):\n",
    "    LOS = []\n",
    "    Type = []\n",
    "    r = requests.get(i)\n",
    "    soup2 = BeautifulSoup(r.content, 'html.parser')\n",
    "    strains = soup2.find_all('a', class_=\"ga_Explore_Strain_Tile\")\n",
    "    for s in strains:\n",
    "        LOS.append(str(s.attrs['href'])[8:])\n",
    "        Type.append(str(s.attrs['href'])[1:7])\n",
    "    z = list(zip(LOS,Type))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strain_dict_entry(strain, stype, user_id, userstars, userreview, straindict):\n",
    "    if straindict is None:\n",
    "        straindict = {}\n",
    "        \n",
    "    if strain not in straindict:\n",
    "        straindict[strain] = {\n",
    "            \"stype\" : stype,\n",
    "            \"user_rev\" : [] \n",
    "        }\n",
    "        \n",
    "    straindict[strain]['user_rev'].append({'user':user_id,\n",
    "                                         'stars':userstars,\n",
    "                                         'review':userreview\n",
    "                                        })\n",
    "    return straindict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs2(d):\n",
    "    \"\"\"Parse the HTML docs that we have stored in a dictionary, return as a list.\n",
    "    Also scrape and parse star rating for each review \"\"\"\n",
    "    strain_text= []\n",
    "    star_rate = []\n",
    "    user_name = []\n",
    "    \n",
    "    soup = BeautifulSoup(d, 'html.parser')\n",
    "    revs = soup.find_all('p',class_='strain-review__text') \n",
    "    for r in revs:\n",
    "        text = r.text\n",
    "        remove_punch = re.sub('[^A-Za-z ]' , \"\" ,text )\n",
    "        token = remove_punch.lower().split()\n",
    "        srm_token = [wnlzer.lemmatize(i) for i in token if not i in set(stopwords.words('english'))]\n",
    "        clean_text = \" \".join(srm_token)\n",
    "        strain_text.append(clean_text)\n",
    "        \n",
    "    tags = soup.select(\"div.div.stars\")\n",
    "    for t in tags:\n",
    "        star = t.attrs['style']\n",
    "        star_rate.append(star_int_conv(star))\n",
    "    \n",
    "    users = soup.find_all('div', class_='strain-review__title')\n",
    "    for u in users:\n",
    "        temp = u.find('h2')\n",
    "        user_name.append(temp.text)\n",
    "        \n",
    "    return star_rate, strain_text, user_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_dummy1(los):\n",
    "    \"\"\"pass in a list of strains to be scraped from Leafly.\n",
    "    returns dictionary keyed by strains w/strain info(reviews) as values\"\"\"\n",
    "    cnt = 0\n",
    "    strain_dict = None\n",
    "    for s in los:\n",
    "        strain = s[0]\n",
    "        stype = s[1]\n",
    "        url = \"https://www.leafly.com/{}/{}/reviews?page=\".format(stype, strain)\n",
    "    \n",
    "        for i in range(1,100):\n",
    "            rev_url=url+str(i)\n",
    "            d = get_reviews2(rev_url)\n",
    "            star, reviews, users = parse_docs2(d)\n",
    "            #print(star)\n",
    "            if len(star) == 0:\n",
    "                break\n",
    "\n",
    "            for userstars, userreview ,user_id in zip(star, reviews, users):\n",
    "                strain_dict = strain_dict_entry(strain, stype, user_id, userstars, userreview, strain_dict)\n",
    "                \n",
    "            \n",
    "            if cnt % 10 == 0:\n",
    "                print(cnt)\n",
    "            cnt +=1\n",
    "            \n",
    "    return strain_dict         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strains = list_o_strains('https://www.leafly.com/explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue-dream', 'hybrid'),\n",
       " ('sour-diesel', 'sativa'),\n",
       " ('gsc', 'hybrid'),\n",
       " ('green-crack', 'sativa'),\n",
       " ('og-kush', 'hybrid'),\n",
       " ('granddaddy-purple', 'indica'),\n",
       " ('original-glue', 'hybrid'),\n",
       " ('white-widow', 'hybrid'),\n",
       " ('blue-dream', 'hybrid'),\n",
       " ('sour-diesel', 'sativa'),\n",
       " ('gsc', 'hybrid'),\n",
       " ('green-crack', 'sativa'),\n",
       " ('og-kush', 'hybrid'),\n",
       " ('granddaddy-purple', 'indica'),\n",
       " ('original-glue', 'hybrid'),\n",
       " ('white-widow', 'hybrid'),\n",
       " ('jack-herer', 'sativa'),\n",
       " ('bubba-kush', 'indica'),\n",
       " ('pineapple-express', 'hybrid'),\n",
       " ('trainwreck', 'hybrid'),\n",
       " ('ak-47', 'hybrid'),\n",
       " ('durban-poison', 'sativa'),\n",
       " ('northern-lights', 'indica'),\n",
       " ('headband', 'hybrid'),\n",
       " ('blue-cheese', 'indica'),\n",
       " ('strawberry-cough', 'sativa'),\n",
       " ('chemdawg', 'hybrid'),\n",
       " ('purple-kush', 'indica'),\n",
       " ('lemon-haze', 'sativa'),\n",
       " ('super-lemon-haze', 'sativa'),\n",
       " ('grape-ape', 'indica'),\n",
       " ('blueberry', 'indica'),\n",
       " ('alaskan-thunder-fuck', 'sativa'),\n",
       " ('super-silver-haze', 'sativa'),\n",
       " ('blackberry-kush', 'indica'),\n",
       " ('cherry-pie', 'hybrid'),\n",
       " ('master-kush', 'indica'),\n",
       " ('skywalker-og', 'hybrid'),\n",
       " ('cheese', 'hybrid'),\n",
       " ('death-star', 'indica'),\n",
       " ('chocolope', 'sativa'),\n",
       " ('amnesia-haze', 'sativa'),\n",
       " ('tahoe-og', 'hybrid'),\n",
       " ('maui-wowie', 'sativa'),\n",
       " ('platinum-gsc', 'hybrid'),\n",
       " ('harlequin', 'sativa'),\n",
       " ('gods-gift', 'indica'),\n",
       " ('la-confidential', 'indica'),\n",
       " ('agent-orange', 'hybrid'),\n",
       " ('purple-urkle', 'indica'),\n",
       " ('lemon-kush', 'hybrid'),\n",
       " ('mazar-x-blueberry', 'hybrid'),\n",
       " ('golden-goat', 'hybrid'),\n",
       " ('afghan-kush', 'indica'),\n",
       " ('dutch-treat', 'hybrid'),\n",
       " ('hindu-kush', 'indica')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Only needs to be ran once\\nd = scraper_dummy1(test_strains)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#Only needs to be ran once\n",
    "d = scraper_dummy1(test_strains)\"\"\"\n",
    "\n",
    "#Already ran it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6b9b4f16b922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_rev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "for k in d.keys():\n",
    "    print(k,len(d[k]['user_rev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-420d29d2d160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gsc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_rev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "d['gsc']['user_rev'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files(d):\n",
    "    cnt = 0\n",
    "    for strain in d.keys():\n",
    "\n",
    "        if not os.path.isdir(os.path.join(target_path, strain)):\n",
    "             os.mkdir(os.path.join(target_path, strain))\n",
    "\n",
    "        for rev in d[strain]['user_rev']:\n",
    "\n",
    "            with open(os.path.join(target_path, strain, strain+'{:05d}.txt'.format(cnt)), mode='w') as strain_file:\n",
    "                strain_file.write(rev['review'])\n",
    "\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_files(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = \"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your]\".split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples 9000\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__max_iter': (10, 50, 80),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__max_features': (None, 5000, 10000, 50000),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 1152 candidates, totalling 5760 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 41.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 72.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 118.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 135.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 186.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5760 out of 5760 | elapsed: 250.3min finished\n",
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 15042.230s\n",
      "\n",
      "Best score: 0.539\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__max_iter: 80\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" # TASK: Build a vectorizer / classifier pipeline that filters out tokens\\n# that are too rare or too frequent\\nclf = Pipeline([\\n    ('vect',TfidfVectorizer(analyzer='character', stop_words=stopwords_)),\\n    ('clf', Perceptron())\\n])\\n\\n# TfidfVectorizer().get_params()\\n# TASK: Build a grid search to find out whether unigrams or bigrams are\\n# more useful.\\n# Fit the pipeline on the training set using grid search for the parameters\\nparameters = {\\n    'vect__ngram_range': [(1, 1), (1, 5), (1,10),(1,20)],\\n    'vect__analyzer': ['word']#,'char']\\n}\\n\\n\\ngs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\\ncclf = gs_clf.fit(docs_train, y_train)\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    dataset = load_files(target_path, shuffle=False)\n",
    "    print('n_samples {}'.format(len(dataset.data)))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.15, random_state=None)\n",
    "\n",
    "    \n",
    "    \n",
    "    # #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (5,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    cclf = grid_search.fit(docs_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"filename = 'finalized_model.sav'\n",
    "joblib.dump(model, filename)\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    clf = Pipeline([\n",
    "        ('vect',TfidfVectorizer(analyzer='character', stop_words=stopwords_)),\n",
    "        ('clf', Perceptron())\n",
    "    ])\n",
    "\n",
    "    # TfidfVectorizer().get_params()\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 5), (1,10),(1,20)],\n",
    "        'vect__analyzer': ['word']#,'char']\n",
    "    }\n",
    "    \n",
    "    \n",
    "    gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "    cclf = gs_clf.fit(docs_train, y_train)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-f2e266f7f410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__analyzer': 'word', 'vect__ngram_range': (1, 5)}\n",
      "0.5911111111111111\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "  .ipynb_checkpoints       0.14      1.00      0.25         1\n",
      "         afghan-kush       0.91      0.83      0.87        12\n",
      "        agent-orange       0.75      0.90      0.82        10\n",
      "               ak-47       0.96      0.87      0.91        30\n",
      "alaskan-thunder-fuck       0.92      0.92      0.92        12\n",
      "        amnesia-haze       0.82      0.85      0.84        27\n",
      "     blackberry-kush       1.00      0.83      0.91        18\n",
      "         blue-cheese       0.60      0.75      0.67        12\n",
      "          blue-dream       0.94      0.95      0.94       150\n",
      "           blueberry       0.75      0.80      0.77        15\n",
      "          bubba-kush       0.94      0.91      0.92        32\n",
      "              cheese       1.00      1.00      1.00         4\n",
      "            chemdawg       1.00      0.81      0.89        21\n",
      "          cherry-pie       0.89      0.94      0.91        17\n",
      "           chocolope       0.92      0.92      0.92        12\n",
      "          death-star       1.00      0.83      0.91        18\n",
      "       durban-poison       0.79      0.93      0.85        28\n",
      "         dutch-treat       0.78      1.00      0.88         7\n",
      "           gods-gift       1.00      0.91      0.95        11\n",
      "         golden-goat       1.00      1.00      1.00        10\n",
      "   granddaddy-purple       0.90      1.00      0.95        62\n",
      "           grape-ape       1.00      0.96      0.98        26\n",
      "         green-crack       0.94      1.00      0.97        73\n",
      "                 gsc       0.97      0.95      0.96        82\n",
      "           harlequin       0.91      0.91      0.91        22\n",
      "            headband       0.91      0.77      0.83        13\n",
      "          hindu-kush       1.00      0.86      0.92        14\n",
      "          jack-herer       0.88      0.85      0.86        33\n",
      "     la-confidential       0.93      0.82      0.87        17\n",
      "          lemon-haze       1.00      1.00      1.00        11\n",
      "          lemon-kush       1.00      1.00      1.00         8\n",
      "         master-kush       0.75      0.90      0.82        10\n",
      "          maui-wowie       1.00      0.95      0.97        19\n",
      "   mazar-x-blueberry       0.20      0.20      0.20         5\n",
      "     northern-lights       0.93      0.85      0.89        33\n",
      "             og-kush       0.95      1.00      0.98        63\n",
      "       original-glue       0.89      0.98      0.94        60\n",
      "   pineapple-express       0.90      0.79      0.84        34\n",
      "        platinum-gsc       1.00      0.50      0.67         8\n",
      "         purple-kush       1.00      0.75      0.86         8\n",
      "        purple-urkle       0.89      0.93      0.91        27\n",
      "        skywalker-og       0.43      0.43      0.43         7\n",
      "         sour-diesel       0.98      0.97      0.98       149\n",
      "    strawberry-cough       0.91      0.95      0.93        22\n",
      "    super-lemon-haze       1.00      1.00      1.00        17\n",
      "   super-silver-haze       1.00      0.75      0.86        20\n",
      "            tahoe-og       1.00      0.86      0.92         7\n",
      "          trainwreck       0.88      1.00      0.93         7\n",
      "         white-widow       0.98      0.96      0.97        46\n",
      "\n",
      "           micro avg       0.92      0.92      0.92      1350\n",
      "           macro avg       0.88      0.87      0.87      1350\n",
      "        weighted avg       0.93      0.92      0.92      1350\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = gs_clf.predict(docs_test)\n",
    "\n",
    "print(gs_clf.best_params_)\n",
    "print(cclf.score(docs_test, y_test))\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "#print(cm)\n",
    "\n",
    "plt.matshow(cm, cmap=plt.cm.cool)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To feel these like (yawn sore tired) try smoking purple-kush\n",
      "To feel these like (smile laugh giggle) try smoking green-crack\n",
      "To feel these like (happy hike ) try smoking green-crack\n",
      "To feel these like (fun happy awake) try smoking blue-dream\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    u'yawn sore tired',\n",
    "    u'smile laugh giggle',\n",
    "    u'happy hike ',\n",
    "    u'fun happy awake'\n",
    "]\n",
    "\n",
    "predicted = cclf.predict(test_strings)\n",
    "\n",
    "for s, p in zip(test_strings, predicted):\n",
    "    print(u'To feel these like ({}) try smoking {}'.format(s, dataset.target_names[p]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
