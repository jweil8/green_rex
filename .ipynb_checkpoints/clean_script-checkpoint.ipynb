{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = pymongo.MongoClient()  # Connect to the MongoDB server using default settings\n",
    "db = mc['strain_reviews']  # Use (or create) a database called 'election_predictions'\n",
    "docs = db['review']  # Use (or create) a collection called 'docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.count_documents(filter={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is how i want the scrape to work\n",
    "\n",
    "    \"\"\"LOS = [List of Strains]\n",
    "        for s in LOS:\n",
    "        url = \"https://www.leafly.com/{}/reviews?page=\".format(s)\n",
    "        for i in range(1,100):\n",
    "            d = get_reviews(url+str(i))\n",
    "            st, rev = parse_docs(d)\n",
    "            if len(st) > 0:\n",
    "               append.(add to your dictionary)\n",
    "            else:\n",
    "                break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_check2(l):\n",
    "    \"\"\"This checks to see if a url is already in my database of reviews. \n",
    "    Only needs to be run in the get reviews function\"\"\"\n",
    "    for url in l:\n",
    "        if docs.find_one({'url':url}):\n",
    "            None\n",
    "        else:\n",
    "            docs.insert_one({'url': url,\n",
    "                 'html': requests.get(url).content\n",
    "                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews2(l):\n",
    "    \"\"\"Pass in a list of URL's and return them in a mongo db table as a dicitonary with \n",
    "    {'url', 'html'} and their corresponding values\"\"\"\n",
    "    r = requests.get(l)\n",
    "    html = (r.content)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stars_list(d):\n",
    "    stars = []\n",
    "    for key, values in d.items():\n",
    "        soup = BeautifulSoup(values, 'html.parser')\n",
    "        tags = soup.select(\"div.div.stars\")\n",
    "        for t in tags:\n",
    "            stars.append(t.attrs['style'])\n",
    "    return stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_int_conv(l):\n",
    "    flat_stars= []\n",
    "    star_num = []\n",
    "    for s in l:\n",
    "        star = int((s[6:].split(';')[0]).strip('px'))\n",
    "        star_num.append((star/22))\n",
    "    return star_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_o_strains(i):\n",
    "    LOS = []\n",
    "    r = requests.get(i)\n",
    "    soup2 = BeautifulSoup(r.content, 'html.parser')\n",
    "    strains = soup2.find_all('a', class_=\"ga_Explore_Strain_Tile\")\n",
    "    for s in strains:\n",
    "        LOS.append(s.attrs['href'])\n",
    "    return LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs2(d):\n",
    "    \"\"\"Parse the HTML docs that we have stored in a dictionary, return as a list.\n",
    "    Also scrape and parse star rating for each review \"\"\"\n",
    "    strain_text= []\n",
    "    star_rate = []\n",
    "    user_name = []\n",
    "    soup = BeautifulSoup(d, 'html.parser')\n",
    "    revs = soup.find_all('p',class_='strain-review__text') \n",
    "    for r in revs:\n",
    "        strain_text.append(r.text)\n",
    "    tags = soup.select(\"div.div.stars\")\n",
    "    star = []\n",
    "    for t in tags:\n",
    "        star.append(t.attrs['style'])\n",
    "    star_rate.append(star_int_conv(star))\n",
    "    users = soup.find_all('div', class_='strain-review__title')\n",
    "    for u in users:\n",
    "        temp = u.find('h2')\n",
    "        user_name.append(temp.text)\n",
    "    return star_rate, strain_text, user_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test works better, is updated!\n",
    "def scraper_dummy1(los):\n",
    "    los_urls = []\n",
    "    rev_urls = []\n",
    "    for s in los:\n",
    "        url = \"https://www.leafly.com{}/reviews?page=\".format(s)\n",
    "        print(url)\n",
    "        for i in range(1,100):\n",
    "            rev_urls=url+str(i)\n",
    "            d = get_reviews2(rev_urls)\n",
    "            star, reviews, users = parse_docs2(d)\n",
    "            print(users)\n",
    "            if len(star[0]) == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_dummy_test(los):\n",
    "    los_urls = []\n",
    "    rev_urls = []\n",
    "    user_list_of_lists = []\n",
    "    star_list_of_lists = []\n",
    "    reviews_list_o_lists = []\n",
    "    #grab URL landing pages for reviews on each strain.\n",
    "    for s in los:\n",
    "        url = \"https://www.leafly.com/{}/reviews?page=\".format(s)\n",
    "        strain_star_list = []\n",
    "        strain_user_list = []\n",
    "        strain_rev_list = []\n",
    "        #get all reviews, stars, and users for each strain in list of strains\n",
    "        for i in range(1,100):\n",
    "            rev_urls=url+str(i)\n",
    "            d = get_reviews2(rev_urls)\n",
    "            star, reviews, users = parse_docs2(d)\n",
    "            #return stars, strains, reviews, in one list, not list of lists for each page scraped. \n",
    "            #star_unlisted = \n",
    "            strain_star_list.extend([x for x in star[0]])\n",
    "            strain_user_list.extend(users)\n",
    "            strain_rev_list.extend(reviews)\n",
    "            if len(star[0]) == 0:\n",
    "                break\n",
    "        #these are long single lists, not list of lists\n",
    "        star_list_of_lists.append(strain_star_list)\n",
    "        user_list_of_lists.append(strain_user_list)\n",
    "        reviews_list_o_lists.append(strain_rev_list)\n",
    "        #make into dictionaries \n",
    "        for s in los:\n",
    "            star_ds = []\n",
    "            rev_ds = []\n",
    "            for i in range(len(star_list_of_lists)):\n",
    "                user_revs =  dict(list(zip(user_list_of_lists[i], reviews_list_o_lists[i])))\n",
    "                rev_ds.append(user_revs)\n",
    "                user_stars = dict(list(zip(user_list_of_lists[i], star_list_of_lists[i])))\n",
    "                star_ds.append(user_stars)\n",
    "    #create Pandas DF of each dictionary as well.\n",
    "    star_dfs = []\n",
    "    rev_dfs = []\n",
    "    for s in star_ds:\n",
    "        star_dfs.append(pd.DataFrame.from_dict(s, orient='index')) \n",
    "    for r in rev_ds: \n",
    "        rev_dfs.append(pd.DataFrame.from_dict(r, orient ='index'))\n",
    "    #create large dataframe\n",
    "    star_df = pd.concat(star_dfs, axis=1, sort=True)\n",
    "    rev_df = pd.concat(rev_dfs, axis=1, sort=True)\n",
    "    rev_df.columns = los\n",
    "    star_df.columns = los\n",
    "    return star_df, rev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strains = list_o_strains('https://www.leafly.com/explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hybrid/original-glue'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strains[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars, reviews = scraper_dummy_test(test_strains[6:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/hybrid/original-glue    4.597222\n",
       "/hybrid/white-widow      4.542857\n",
       "dtype: float64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars.aggregate('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
