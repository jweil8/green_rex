{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "source_path = 'test_data'\n",
    "#source_file = 'test_strains_reviews.csv'\n",
    "target_path = '/Users/jordanweil/green_rex/test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('test_data/test_strains_reviews.csv')\n",
    "r = reviews.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews2(l):\n",
    "    \"\"\"Pass in a list of URL's and return them in a mongo db table as a dicitonary with \n",
    "    {'url', 'html'} and their corresponding values\"\"\"\n",
    "    r = requests.get(l)\n",
    "    html = (r.content)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_stars_list(d):\n",
    "#     stars = []\n",
    "#     for key, values in d.items():\n",
    "#         soup = BeautifulSoup(values, 'html.parser')\n",
    "#         tags = soup.select(\"div.div.stars\")\n",
    "#         for t in tags:\n",
    "#             stars.append(t.attrs['style'])\n",
    "#     return stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_int_conv(s):\n",
    "    star = (int((s[6:].split(';')[0]).strip('px'))/22)\n",
    "    \n",
    "    return star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_o_strains(i):\n",
    "    LOS = []\n",
    "    Type = []\n",
    "    r = requests.get(i)\n",
    "    soup2 = BeautifulSoup(r.content, 'html.parser')\n",
    "    strains = soup2.find_all('a', class_=\"ga_Explore_Strain_Tile\")\n",
    "    for s in strains:\n",
    "        LOS.append(str(s.attrs['href'])[8:])\n",
    "        Type.append(str(s.attrs['href'])[1:7])\n",
    "    z = list(zip(LOS,Type))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strain_dict_entry(strain, stype, user_id, userstars, userreview, straindict):\n",
    "    if straindict is None:\n",
    "        straindict = {}\n",
    "        \n",
    "    if strain not in straindict:\n",
    "        straindict[strain] = {\n",
    "            \"stype\" : stype,\n",
    "            \"user_rev\" : [] \n",
    "        }\n",
    "        \n",
    "    straindict[strain]['user_rev'].append({'user':user_id,\n",
    "                                         'stars':userstars,\n",
    "                                         'review':userreview\n",
    "                                        })\n",
    "    return straindict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs2(d):\n",
    "    \"\"\"Parse the HTML docs that we have stored in a dictionary, return as a list.\n",
    "    Also scrape and parse star rating for each review \"\"\"\n",
    "    strain_text= []\n",
    "    star_rate = []\n",
    "    user_name = []\n",
    "    \n",
    "    soup = BeautifulSoup(d, 'html.parser')\n",
    "    revs = soup.find_all('p',class_='strain-review__text') \n",
    "    for r in revs:\n",
    "        strain_text.append(r.text)\n",
    "        \n",
    "    tags = soup.select(\"div.div.stars\")\n",
    "    for t in tags:\n",
    "        star = t.attrs['style']\n",
    "        star_rate.append(star_int_conv(star))\n",
    "    \n",
    "    users = soup.find_all('div', class_='strain-review__title')\n",
    "    for u in users:\n",
    "        temp = u.find('h2')\n",
    "        user_name.append(temp.text)\n",
    "        \n",
    "    return star_rate, strain_text, user_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_dummy1(los):\n",
    "    \"\"\"pass in a list of strains to be scraped from Leafly.\n",
    "    returns dictionary keyed by strains w/strain info(reviews) as values\"\"\"\n",
    "    cnt = 0\n",
    "    strain_dict = None\n",
    "    for s in los:\n",
    "        strain = s[0]\n",
    "        stype = s[1]\n",
    "        url = \"https://www.leafly.com/{}/{}/reviews?page=\".format(stype, strain)\n",
    "    \n",
    "        for i in range(1,100):\n",
    "            rev_url=url+str(i)\n",
    "            d = get_reviews2(rev_url)\n",
    "            star, reviews, users = parse_docs2(d)\n",
    "            #print(star)\n",
    "            if len(star) == 0:\n",
    "                break\n",
    "\n",
    "            for userstars, userreview ,user_id in zip(star, reviews, users):\n",
    "                strain_dict = strain_dict_entry(strain, stype, user_id, userstars, userreview, strain_dict)\n",
    "                \n",
    "            \n",
    "            if cnt % 10 == 0:\n",
    "                print(cnt)\n",
    "            cnt +=1\n",
    "            \n",
    "    return strain_dict         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strains = list_o_strains('https://www.leafly.com/explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue-dream', 'hybrid'),\n",
       " ('sour-diesel', 'sativa'),\n",
       " ('gsc', 'hybrid'),\n",
       " ('green-crack', 'sativa')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test_strains[:4]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "d = scraper_dummy1(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue-dream 436\n",
      "sour-diesel 502\n",
      "gsc 280\n",
      "green-crack 243\n"
     ]
    }
   ],
   "source": [
    "for k in d.keys():\n",
    "    print(k,len(d[k]['user_rev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 'BenchAdventures',\n",
       " 'stars': 5.0,\n",
       " 'review': '\"This is what I\\'ve been smoking recently when I don\\'t need to \"do\" anything. If you\\'re like me, you\\'ll want to make sure the floor is clear of tripping hazards, your laptop is away from your water, etc.\\nSet out some snacks before you start smoking, and make sure you\\'re in a safe environment.\\nHybrids generally put me in the couch more than Indica, but GSC makes fine motor skills especially hard. It\\'s also especially de...\"'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['gsc']['user_rev'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files(d):\n",
    "    cnt = 0\n",
    "    for strain in d.keys():\n",
    "\n",
    "        if not os.path.isdir(os.path.join(target_path, strain)):\n",
    "             os.mkdir(os.path.join(target_path, strain))\n",
    "\n",
    "        for rev in d[strain]['user_rev']:\n",
    "\n",
    "            with open(os.path.join(target_path, strain, strain+'{:05d}.txt'.format(cnt)), mode='w') as strain_file:\n",
    "                strain_file.write(rev['review'])\n",
    "\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_files(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = \"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your]\".split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples 1463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['your'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/jordanweil/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "    dataset = load_files(target_path, shuffle=False)\n",
    "    print('n_samples {}'.format(len(dataset.data)))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    clf = Pipeline([\n",
    "        ('vect',TfidfVectorizer(analyzer='character', stop_words=stopwords_)),\n",
    "        ('clf', Perceptron())\n",
    "    ])\n",
    "\n",
    "    # TfidfVectorizer().get_params()\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 5), (1,10),(1,20)],\n",
    "        'vect__analyzer': ['word']#,'char']\n",
    "    }\n",
    "    gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "    cclf = gs_clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__analyzer': 'word', 'vect__ngram_range': (1, 5)}\n",
      "0.4808743169398907\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      ".ipynb_checkpoints       0.00      0.00      0.00         1\n",
      "        blue-dream       0.45      0.64      0.53       111\n",
      "       green-crack       0.38      0.17      0.24        52\n",
      "               gsc       0.58      0.35      0.43        75\n",
      "       sour-diesel       0.56      0.55      0.55       127\n",
      "\n",
      "         micro avg       0.48      0.48      0.48       366\n",
      "         macro avg       0.39      0.34      0.35       366\n",
      "      weighted avg       0.50      0.48      0.47       366\n",
      "\n",
      "[[ 0  1  0  0  0]\n",
      " [ 4 71  5  7 24]\n",
      " [ 4 17  9  4 18]\n",
      " [ 2 30  3 26 14]\n",
      " [ 3 39  7  8 70]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TASK: Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = gs_clf.predict(docs_test)\n",
    "\n",
    "print(gs_clf.best_params_)\n",
    "print(cclf.score(docs_test, y_test))\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm, cmap=plt.cm.cool)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class of trippy  fun, hyphy is blue-dream\n",
      "The class of high, smile, amazing is blue-dream\n",
      "The class of happy, hike, awake is green-crack\n",
      "The class of giggly, happy, fun is .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    u'trippy  fun, hyphy',\n",
    "    u'high, smile, amazing',\n",
    "    u'happy, hike, awake',\n",
    "    u'giggly, happy, fun'\n",
    "]\n",
    "\n",
    "predicted = cclf.predict(test_strings)\n",
    "\n",
    "for s, p in zip(test_strings, predicted):\n",
    "    print(u'The class of {} is {}'.format(s, dataset.target_names[p]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
